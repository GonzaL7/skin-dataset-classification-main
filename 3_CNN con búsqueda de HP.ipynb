{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ce9329-dfa9-4c9a-a759-43075605a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ccca9e-4fec-44af-a8b7-d034101cb913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.7'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import io\n",
    "from helper import *\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba3ee6d-5ae2-4750-b49b-b343f19c9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb85e50-d8ea-4a0d-82b8-dfbfa12ef379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fdb67cc-e660-4822-a999-ff80aa316a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b1b529-ba09-4afc-a901-b04b3c07ed00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/glouz/Documents/ITBA/4to_Ano/2Q/Redes/skin-dataset-classification-main/mlruns/814691573414303420', creation_time=1764114177954, experiment_id='814691573414303420', last_update_time=1764114177954, lifecycle_stage='active', name='Clasificador_Imagenes', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Clasificador_Imagenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3135b655-4342-4c85-9c68-46ef459de7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_classification_report(model, loader, writer, device, classes, step, prefix=\"val\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    fig_cm, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "    ax.set_title(f'{prefix.title()} - Confusion Matrix')\n",
    "\n",
    "    # Guardar localmente y subir a MLflow\n",
    "    fig_path = f\"confusion_matrix_{prefix}_epoch_{step}.png\"\n",
    "    fig_cm.savefig(fig_path)\n",
    "    mlflow.log_artifact(fig_path)\n",
    "    os.remove(fig_path)\n",
    "\n",
    "    plot_to_tensorboard(fig_cm, writer, f\"{prefix}/confusion_matrix\", step)\n",
    "\n",
    "    cls_report = classification_report(all_labels, all_preds, target_names=classes)\n",
    "    writer.add_text(f\"{prefix}/classification_report\", f\"<pre>{cls_report}</pre>\", step)\n",
    "\n",
    "    # También loguear texto del reporte\n",
    "    with open(f\"classification_report_{prefix}_epoch_{step}.txt\", \"w\") as f:\n",
    "        f.write(cls_report)\n",
    "    mlflow.log_artifact(f.name)\n",
    "    os.remove(f.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a152ed-69db-408c-9920-980b864416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y validación\n",
    "def evaluate(model, loader, writer, device, classes, epoch=None, prefix=\"val\"):\n",
    "    log_classification_report(model, loader, writer, device, classes, step=epoch , prefix=\"val\")\n",
    "    model.eval()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Loguear imágenes del primer batch\n",
    "            if i == 0 and epoch is not None:\n",
    "                img_grid = vutils.make_grid(images[:8].cpu(), normalize=True)\n",
    "                writer.add_image(f\"{prefix}/images\", img_grid, global_step=epoch)\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    avg_loss = loss_sum / len(loader)\n",
    "\n",
    "    if epoch is not None:\n",
    "        writer.add_scalar(f\"{prefix}/loss\", avg_loss, epoch)\n",
    "        writer.add_scalar(f\"{prefix}/accuracy\", acc, epoch)\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2acfa65-34ab-454c-ba1a-b2f8f1f99c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_dir = \"data/Split_smol/train\"\n",
    "val_dir = \"data/Split_smol/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86d4782-1460-4d63-b282-14ac74d1587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de logs de tensorboard\n",
    "log_dir = \"runs/experimento_skin\"\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7b2f89-33be-4c4f-a499-8704987249ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8895635775806027"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b58ed-4d28-431f-a8be-2549a20402cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hparams_space= {\n",
    "    \"model\": (\"CNNClassifier\"),\n",
    "    \"input_size\":  [32,64,128],\n",
    "    \"batch_size\": [16,64,128],\n",
    "    \"lr\": [1e-2,1e-3,1e-4],\n",
    "    \"epochs\": 200,\n",
    "    \"optimizer\":  [\"Adam\", \"SGD\"],\n",
    "    \"HFlip\": [0.0,0.5],\n",
    "    \"VFlip\": [0.0,0.5],\n",
    "    \"RBContrast\": [0.0, 0.5],\n",
    "    \"loss_fn\": \"CrossEntropyLoss\",\n",
    "    \"train_dir\": train_dir,\n",
    "    \"val_dir\": val_dir,\n",
    "    \"es_patience\": 5,\n",
    "    \"dropout\": [0.0, 0.1,0.2,0.3],\n",
    "}\"\"\"\n",
    "hparams_space = {\n",
    "    \"model\": (\"CNNClassifier\"),\n",
    "    \"input_size\":  [32,64],              # antes: [32,64,128]\n",
    "    \"batch_size\": [16, 32],           # antes: [16,64,128]\n",
    "    \"lr\": [1e-3, 1e-4],               # sacar 1e-2 que diverge a veces\n",
    "    \"epochs\": 100,                     # antes: 200\n",
    "    \"optimizer\":  [\"Adam\"],           # sacar SGD para reducir runs\n",
    "    \"HFlip\": [0.0, 0.5],\n",
    "    \"VFlip\": [0.0],                   # sacar VFlip, no aporta mucho\n",
    "    \"RBContrast\": [0.0, 0.5],\n",
    "    \"loss_fn\": \"CrossEntropyLoss\",\n",
    "    \"train_dir\": train_dir,\n",
    "    \"val_dir\": val_dir,\n",
    "    \"es_patience\": 10,                # dejar más paciencia\n",
    "    \"dropout\": [0.0, 0.2, 0.3],       # sacar 0.1 que casi no cambia\n",
    "}\n",
    "#Se ajustó los parámetros para que el tiempo de computo sea accesible con mi computadora. Las variaciones fueron sugeridas por ChatGPT y revisadas por mi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1097b11-4a9a-402e-b238-6acfe506a76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo número: 0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 00:25:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:26:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:27:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:27:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:27:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:28:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:28:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:29:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:30:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n",
      "modelo número: 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 00:32:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:33:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:34:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:34:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:35:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:36:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:38:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:39:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n",
      "modelo número: 2\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 00:41:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:42:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/26 00:44:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     64\u001b[39m correct, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\Documents\\ITBA\\4to_Ano\\2Q\\Redes\\skin-dataset-classification-main\\helper.py:65\u001b[39m, in \u001b[36mCustomImageDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     62\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     augmented = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     image = augmented[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albumentations\\core\\composition.py:610\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, force_apply, *args, **data)\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28mself\u001b[39m.preprocess(data)\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     data = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    611\u001b[39m     \u001b[38;5;28mself\u001b[39m._track_transform_params(t, data)\n\u001b[32m    612\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.check_data_post_transform(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albumentations\\core\\transforms_interface.py:273\u001b[39m, in \u001b[36mBasicTransform.__call__\u001b[39m\u001b[34m(self, force_apply, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.deterministic:\n\u001b[32m    272\u001b[39m         kwargs[\u001b[38;5;28mself\u001b[39m.save_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] = deepcopy(params)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albumentations\\core\\transforms_interface.py:310\u001b[39m, in \u001b[36mBasicTransform.apply_with_params\u001b[39m\u001b[34m(self, params, *args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    308\u001b[39m         target_function = \u001b[38;5;28mself\u001b[39m._key2func[key]\n\u001b[32m    309\u001b[39m         res[key] = ensure_contiguous_output(\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m             \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_contiguous_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    311\u001b[39m         )\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    313\u001b[39m     res[key] = arg\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albumentations\\augmentations\\pixel\\transforms.py:246\u001b[39m, in \u001b[36mNormalize.apply\u001b[39m\u001b[34m(self, img, **params)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Apply normalization to the input image.\u001b[39;00m\n\u001b[32m    236\u001b[39m \n\u001b[32m    237\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.normalization == \u001b[33m\"\u001b[39m\u001b[33mstandard\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m normalize_per_image(img, \u001b[38;5;28mself\u001b[39m.normalization)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\functions.py:330\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(img, mean, denominator)\u001b[39m\n\u001b[32m    328\u001b[39m mean = convert_value(mean, num_channels)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m img.dtype == np.uint8:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize_lut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m normalize_opencv(img, mean, denominator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\decorators.py:42\u001b[39m, in \u001b[36mpreserve_channel_dim.<locals>.wrapped_function\u001b[39m\u001b[34m(img, *args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_function\u001b[39m(img: np.ndarray, *args: P.args, **kwargs: P.kwargs) -> np.ndarray:\n\u001b[32m     41\u001b[39m     shape = img.shape\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) == NUM_MULTI_CHANNEL_DIMENSIONS \u001b[38;5;129;01mand\u001b[39;00m shape[-\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result.ndim == MONO_CHANNEL_DIMENSIONS:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.expand_dims(result, axis=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\glouz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\functions.py:322\u001b[39m, in \u001b[36mnormalize_lut\u001b[39m\u001b[34m(img, mean, denominator)\u001b[39m\n\u001b[32m    318\u001b[39m     mean = mean.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    320\u001b[39m luts = (np.arange(\u001b[32m0\u001b[39m, max_value + \u001b[32m1\u001b[39m, dtype=np.float32) - mean) * denominator\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv2.merge([cv2.LUT(img[:, :, i], luts[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m)\u001b[49m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "modelnbr = 0\n",
    "for input_size in hparams_space[\"input_size\"]:\n",
    "    for batch_size in hparams_space[\"batch_size\"]:\n",
    "        for lr in hparams_space[\"lr\"]:\n",
    "            for optimizer in hparams_space[\"optimizer\"]:\n",
    "                for HFlip in hparams_space[\"HFlip\"]:\n",
    "                    for VFlip in hparams_space[\"VFlip\"]:\n",
    "                        for RBContrast in hparams_space[\"RBContrast\"]:\n",
    "                            for dropout in hparams_space[\"dropout\"]:\n",
    "                                if np.random.rand() < 0.07:\n",
    "                                    print(f\"modelo número: {modelnbr}\", end = \"\\r\")\n",
    "                                    modelnbr += 1\n",
    "                                    hparams= {\n",
    "                                        \"model\": (\"CNNClassifier\"),\n",
    "                                        \"input_size\":  input_size,\n",
    "                                        \"batch_size\": batch_size,\n",
    "                                        \"lr\": lr,\n",
    "                                        \"epochs\": 100,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"HFlip\": HFlip,\n",
    "                                        \"VFlip\": VFlip,\n",
    "                                        \"RBContrast\": RBContrast,\n",
    "                                        \"loss_fn\": \"CrossEntropyLoss\",\n",
    "                                        \"train_dir\": train_dir,\n",
    "                                        \"val_dir\": val_dir,\n",
    "                                        \"es_patience\": 10,\n",
    "                                        \"dropout\": dropout,\n",
    "                                    }\n",
    "                                    train_transform = A.Compose([\n",
    "                                        A.Resize(hparams[\"input_size\"], hparams[\"input_size\"]),\n",
    "                                        A.HorizontalFlip(p=hparams[\"HFlip\"]),\n",
    "                                        A.VerticalFlip(p=hparams[\"VFlip\"]),\n",
    "                                        A.RandomBrightnessContrast(p=hparams[\"RBContrast\"]),\n",
    "                                        A.Normalize(),\n",
    "                                        ToTensorV2()\n",
    "                                    ])\n",
    "                                    val_test_transform = A.Compose([\n",
    "                                        A.Resize(hparams[\"input_size\"], hparams[\"input_size\"]),\n",
    "                                        A.Normalize(),\n",
    "                                        ToTensorV2()\n",
    "                                    ])\n",
    "                                    train_dataset = CustomImageDataset(train_dir, transform=train_transform)\n",
    "                                    val_dataset   = CustomImageDataset(val_dir, transform=val_test_transform)\n",
    "                                    batch_size = hparams[\"batch_size\"]\n",
    "                                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                                    val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
    "                                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                                    num_classes = len(set(train_dataset.labels))\n",
    "                                    model = CNNClassifier(num_classes=num_classes, input_size = hparams[\"input_size\"], dropout = hparams[\"dropout\"]).to(device)\n",
    "                                    criterion = nn.CrossEntropyLoss()\n",
    "                                    optimizer = optim.Adam(model.parameters(), lr=hparams[\"lr\"]) if hparams[\"optimizer\"]==\"Adam\" else optim.SGD(model.parameters(), lr=hparams[\"lr\"])\n",
    "                                    hparams[\"count_params\"] = count_parameters(model)\n",
    "                                    with mlflow.start_run():\n",
    "                                        # Log hiperparámetros\n",
    "                                        mlflow.log_params(hparams)\n",
    "                                        best_val_acc = 0\n",
    "                                        best_val_loss = 0\n",
    "                                        best_train_acc = 0\n",
    "                                        best_train_loss = 0\n",
    "                                        best_epoch = 0\n",
    "                                        for epoch in range(hparams[\"epochs\"]):\n",
    "                                            model.train()\n",
    "                                            running_loss = 0.0\n",
    "                                            correct, total = 0, 0\n",
    "                                        \n",
    "                                            for images, labels in train_loader:\n",
    "                                                images, labels = images.to(device), labels.to(device)\n",
    "                                        \n",
    "                                                optimizer.zero_grad()\n",
    "                                                outputs = model(images)\n",
    "                                                loss = criterion(outputs, labels)\n",
    "                                                loss.backward()\n",
    "                                                optimizer.step()\n",
    "                                        \n",
    "                                                running_loss += loss.item()\n",
    "                                                _, preds = torch.max(outputs, 1)\n",
    "                                                correct += (preds == labels).sum().item()\n",
    "                                                total += labels.size(0)\n",
    "                                        \n",
    "                                            train_loss = running_loss / len(train_loader)\n",
    "                                            train_acc = 100.0 * correct / total\n",
    "                                            val_loss, val_acc = evaluate(model, val_loader, writer, device,train_dataset.label_encoder.classes_,epoch=epoch, prefix=\"val\")\n",
    "                                        \n",
    "                                            #print(f\"Epoch {epoch+1}:\")\n",
    "                                            #print(f\"  Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "                                            #print(f\"  Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "                                        \n",
    "                                            writer.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "                                            writer.add_scalar(\"train/accuracy\", train_acc, epoch)\n",
    "                                        \n",
    "                                            # Log en MLflow\n",
    "                                            mlflow.log_metrics({\n",
    "                                                \"train_loss\": train_loss,\n",
    "                                                \"train_accuracy\": train_acc,\n",
    "                                                \"val_loss\": val_loss,\n",
    "                                                \"val_accuracy\": val_acc\n",
    "                                            }, step=epoch)\n",
    "                                            if val_acc > best_val_acc:\n",
    "                                                best_val_acc = val_acc\n",
    "                                                best_val_loss = val_loss\n",
    "                                                best_train_acc = train_acc\n",
    "                                                best_train_loss = train_loss\n",
    "                                                best_epoch = epoch\n",
    "                                                # Guardar modelo\n",
    "                                                torch.save(model.state_dict(), \"cnn_model.pth\")\n",
    "                                                #print(\"Modelo guardado como 'cnn_model.pth'\")\n",
    "                                                mlflow.log_artifact(\"cnn_model.pth\")\n",
    "                                                mlflow.pytorch.log_model(model, artifact_path=\"pytorch_model\")\n",
    "                                            elif epoch > best_epoch + hparams[\"es_patience\"]:\n",
    "                                                print(\"Early Stopping\")\n",
    "                                                break\n",
    "                                                \n",
    "                                        mlflow.log_metrics({\n",
    "                                                \"train_loss\": best_train_loss,\n",
    "                                                \"train_accuracy\": best_train_acc,\n",
    "                                                \"val_loss\": best_val_loss,\n",
    "                                                \"val_accuracy\": best_val_acc,\n",
    "                                                \"best_epoch\": best_epoch\n",
    "                                            }, step=epoch+1)                                                \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585580ed-3db0-4687-8ab5-a09abacc621b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594da669-b48e-4307-bd29-11cd988fb4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34241d72-902a-491a-a1e5-a6ec756ab54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
